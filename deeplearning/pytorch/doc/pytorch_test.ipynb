{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from skimage import io, transform\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 PyTorch 张量    \n",
    "\n",
    "PyTorch 的关键数据结构是张量，即多维数组。其功能与 NumPy 的 ndarray 对象类似，如下我们可以使用 torch.Tensor() 创建张量   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'torch.FloatTensor'> and size:  torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "# Generate a 2-D pytorch tensor (i.e., a matrix)\n",
    "pytorch_tensor = torch.Tensor(10, 20)\n",
    "print(\"type: \", type(pytorch_tensor), \"and size: \", pytorch_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你需要一个兼容 NumPy 的表征，或者你想从现有的 NumPy 对象中创建一个 PyTorch 张量，那么就很简单了。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'numpy.ndarray'> and size:  (10, 20)\n",
      "type:  <class 'torch.FloatTensor'> and size:  torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "# Convert the pytorch tensor to a numpy array:\n",
    "numpy_tensor = pytorch_tensor.numpy()\n",
    "print(\"type: \", type(numpy_tensor), \"and size: \", numpy_tensor.shape)\n",
    "\n",
    "# Convert the numpy array to Pytorch Tensor:\n",
    "pytorch_tensor_from_numpy = torch.Tensor(numpy_tensor)\n",
    "print(\"type: \", type(pytorch_tensor_from_numpy), \"and size: \", pytorch_tensor_from_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PyTorch vs. NumPy   \n",
    "\n",
    "PyTorch 并不是 NumPy 的简单替代品，但它实现了很多 NumPy 功能。其中有一个不便之处是其命名规则，有时候它和 NumPy 的命名方法相当不同。我们来举几个例子说明其中的区别：     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[0, 1:3, :, 4]: \n",
      " \n",
      " 0.1894  0.0223  0.6835\n",
      " 0.1893  0.7526  0.5745\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "NdArray[0, 1:3, :, 4]: \n",
      " [[0.18937165 0.02234507 0.6834594 ]\n",
      " [0.18930644 0.7526294  0.57448363]]\n",
      "\n",
      "\n",
      "\n",
      "Tensor Masked[ > 0]: \n",
      " \n",
      " 0.0642\n",
      " 0.1557\n",
      " 0.2599\n",
      " 0.1114\n",
      " 0.3491\n",
      " 0.4310\n",
      " 0.4877\n",
      " 0.4954\n",
      " 0.4013\n",
      " 0.4260\n",
      " 0.0523\n",
      " 0.2934\n",
      " 0.2819\n",
      " 0.4429\n",
      " 0.0061\n",
      " 0.1835\n",
      " 0.3229\n",
      " 0.3450\n",
      " 0.0973\n",
      " 0.4545\n",
      " 0.2526\n",
      " 0.3362\n",
      " 0.1556\n",
      " 0.1637\n",
      " 0.0745\n",
      " 0.4157\n",
      " 0.0460\n",
      " 0.4945\n",
      " 0.1632\n",
      " 0.2955\n",
      " 0.0083\n",
      " 0.3109\n",
      " 0.3672\n",
      " 0.4373\n",
      " 0.1461\n",
      " 0.1979\n",
      " 0.3489\n",
      " 0.2996\n",
      " 0.3384\n",
      " 0.3077\n",
      " 0.1458\n",
      " 0.3778\n",
      " 0.2888\n",
      " 0.2980\n",
      " 0.4427\n",
      " 0.4778\n",
      " 0.4666\n",
      " 0.0463\n",
      " 0.1463\n",
      " 0.3214\n",
      " 0.2673\n",
      " 0.2353\n",
      " 0.3770\n",
      " 0.4079\n",
      " 0.0747\n",
      " 0.4805\n",
      " 0.3928\n",
      " 0.1455\n",
      " 0.1484\n",
      " 0.1588\n",
      " 0.1095\n",
      " 0.1398\n",
      " 0.1123\n",
      " 0.3116\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "NdArray Masked[ > 0]: \n",
      " [0.06424862 0.1557194  0.25991428 0.11136186 0.3491289  0.43095034\n",
      " 0.4877271  0.49543792 0.40133327 0.42603815 0.05229646 0.29344392\n",
      " 0.2818557  0.4429251  0.00611085 0.1834594  0.32294118 0.34498173\n",
      " 0.09730124 0.4545083  0.2526294  0.33621    0.15560937 0.16372925\n",
      " 0.07448363 0.41568398 0.04603642 0.49446535 0.16322094 0.29552746\n",
      " 0.00831527 0.31087375 0.36718667 0.43732446 0.14610153 0.19790077\n",
      " 0.3488565  0.29962337 0.3384441  0.3076954  0.14579976 0.3778187\n",
      " 0.28884798 0.2979614  0.4427439  0.4778391  0.46663976 0.04627681\n",
      " 0.14626837 0.32139808 0.26726812 0.23526049 0.37698615 0.40786165\n",
      " 0.07465619 0.48050386 0.3928281  0.14545196 0.14835024 0.15875018\n",
      " 0.10945815 0.13979071 0.11226034 0.3116333 ]\n"
     ]
    }
   ],
   "source": [
    "# 1 张量创建 \n",
    "t = torch.rand(2, 4, 3, 5)\n",
    "a = np.random.rand(2,4,3,5)\n",
    "\n",
    "# 2 张量分割  \n",
    "a = t.numpy()\n",
    "pytorch_slice = t[0, 1:3, :, 4]\n",
    "numpy_slice = a[0, 1:3, :, 4]\n",
    "print('Tensor[0, 1:3, :, 4]: \\n', pytorch_slice)\n",
    "print('NdArray[0, 1:3, :, 4]: \\n', numpy_slice)\n",
    "\n",
    "# 3 张量 Masking \n",
    "t = t- 0.5\n",
    "a = t.numpy()\n",
    "pytorch_masked = t[t > 0]\n",
    "numpy_masked = a[a > 0]\n",
    "print('\\n\\n')\n",
    "print('Tensor Masked[ > 0]: \\n', pytorch_masked)\n",
    "print('NdArray Masked[ > 0]: \\n', numpy_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PyTorch 变量   \n",
    "\n",
    "- PyTorch 张量的简单封装    \n",
    "- 帮助建立计算图   \n",
    "- Autograd（自动微分库）的必要部分   \n",
    "- 将关于这些变量的梯度保存在 .grad 中    \n",
    "\n",
    "![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic5fEM0JbibKGU6dqREn9g9ntLluCic3CHpXCJFicqORAbicHt4pD0sPG8FB8PxvJhShAaAP976raGaYA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n",
    "\n",
    "结构图:    \n",
    "\n",
    "![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic5fEM0JbibKGU6dqREn9g9nwTAEvHBLXP48kXqAcS9icibm9HJPKdyUBricz9jvygicLic7FiblofSkA2Fg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n",
    "\n",
    "计算图和变量：在 PyTorch 中，神经网络会使用相互连接的变量作为计算图来表示。PyTorch 允许通过代码构建计算图来构建网络模型；之后 PyTorch 会简化估计模型权重的流程，例如通过自动计算梯度的方式。   \n",
    "\n",
    "举例来说，假设我们想构建两层模型，那么首先要为输入和输出创建张量变量。我们可以将 PyTorch Tensor 包装进 Variable 对象中：   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = Variable(torch.randn(4, 1), requires_grad = False)\n",
    "y = Variable(torch.randn(2, 1), requires_grad = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把 requires_grad 设置为 True，表明我们想要自动计算梯度，这将用于反向传播中以优化权重。   \n",
    "\n",
    "现在我们来定义权重：   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Variable(torch.randn(5, 4), requires_grad = True)\n",
    "w2 = Variable(torch.randn(2, 5), requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0075 -0.1060  1.6483  1.3925\n",
      " 0.9572  0.4237  0.2569 -0.0794\n",
      "-0.9989  0.7801  1.3199 -0.1410\n",
      " 1.0551 -0.7859  0.4443  1.6365\n",
      " 1.7632  0.1500  0.6724 -0.3000\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n",
      "torch.Size([5, 4])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def model_forward(x):\n",
    "    return F.sigmoid(w2 @ F.sigmoid( w1 @ x))\n",
    "\n",
    "# 打印初始化数据\n",
    "print(w1)\n",
    "print(w1.data.shape)\n",
    "print(w1.grad)    # Initially, non-existent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 PyTorch 反向传播   \n",
    "\n",
    "这样我们有了输入和目标、模型权重，那么是时候训练模型了。我们需要三个组件：   \n",
    "\n",
    "(1) 损失函数：描述我们模型的预测距离目标还有多远；   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 优化算法：用于更新权重；    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimier = optim.SGD([w1, w2], lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 反向传播步骤：   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0077 -0.1059  1.6494  1.3938\n",
      " 0.9574  0.4238  0.2580 -0.0779\n",
      "-0.9990  0.7800  1.3193 -0.1417\n",
      " 1.0551 -0.7859  0.4444  1.6366\n",
      " 1.7632  0.1500  0.6723 -0.3001\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    loss = criterion(model_forward(x), y)\n",
    "    optimier.zero_grad()   # Zero-out previous gradients\n",
    "    loss.backward()    # Compute new gradients\n",
    "    optimier.step()    # Apply these gradients\n",
    "print (w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 PyTorch CUDA 接口   \n",
    "\n",
    "PyTorch 的优势之一是为张量和 autograd 库提供 CUDA 接口。使用 CUDA GPU，你不仅可以加速神经网络训练和推断，还可以加速任何映射至 PyTorch 张量的工作负载。  \n",
    "\n",
    "你可以调用 torch.cuda.is_available() 函数，检查 PyTorch 中是否有可用 CUDA。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, you have a GPU!\n"
     ]
    }
   ],
   "source": [
    "cuda_gpu = torch.cuda.is_available()\n",
    "if cuda_gpu:\n",
    "    print(\"Great, you have a GPU!\")\n",
    "else:\n",
    "    print(\"Life is short -- consider a GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.cuda()**   \n",
    "很好，现在你有 GPU 了。      \n",
    "\n",
    "之后，使用 cuda 加速代码就和调用一样简单。如果你在张量上调用 .cuda()，则它将执行从 CPU 到 CUDA GPU 的数据迁移。如果你在模型上调用 .cuda()，则它不仅将所有内部储存移到 GPU，还将整个计算图映射至 GPU。    \n",
    "\n",
    "要想将张量或模型复制回 CPU，比如想和 NumPy 交互，你可以调用 .cpu()。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
