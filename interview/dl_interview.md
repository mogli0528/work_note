# `deepLearning`面试问题

1. 良好的C/C++程序开发能力， 熟悉linux, python, matlab, opencv等开发工具;
2. 具备良好的学习能力，能够训练阅读英文文献，并根据文献快速实现相关算法;
3. 熟悉图像滤波，图像增强，边缘检测，特征提取等传统图像处理算法，同时对深度学习CNN有深刻的理解;
4. 熟练tensorflow/pytorch/caffe等一种或多种深度学习框架;
5. 掌握爬虫技术优化。
6. 具有人脸识别项目优先;


## 理论方面：        
1.书写BP算法推导过程；        
2.书写softmax损失函数            
3.书写交叉熵损失函数                   
4.书写A-softmax损失函数              
5.书写triplet loss损失函数               
6.书写center loss损失函数                   
7.书写感受野推导公式                 
8.书写网络结构推导公式；             
9.caffe是怎么计算梯度；               
10.网络模型参数估计方法；
11.怎样设计网络；              
12.caffe怎么添加层，过程有那些；           
13.BN 层原理以及公式书写，test时设置false默认（minibatch计算），train时设置true（整个网络计算）；          
14.ResNet为什么可以做到这么深，原理是什么。           

## 项目方面        
13.训练数据集多大，怎么处理和清洗数据，数据怎么准备；       
14.CNN网络的测试准确率多少；          
15.CNN网络层数多大，有哪些组成；            
16.faster rcnn原理和注意事项；                
18.阅读英文文献多么，2017年ICCV和CVPR论文关注点是那些；                
19.模型压缩方法有哪些，你用过那些网络；           
20.是否了解LSTM RNN；            
21.怎么优化网络，做过优化有那些。              

mobile net  
shuffe net  

## 吴恩达课程中回答的问题    

2. 为什么要进行实例化探究?   
通过学习别人现有的成熟网络框架,将其应用到自己的项目中去.   
3. ResNet 的核心思想   
- 将 a[l] 的信息直接传送到 z[l+2] 进行 ReLU 非线性激活之前(线性激活之后)的位置.
- 有利于解决梯度消失和梯度爆炸问题.    
4. 为什么 ResNet 会表现的这么好?    
首先它不会使网络性能受到影响.即使梯度消失,还是可以使用 shortcut 传过来的值计算出梯度进行训练,因此它非常容易学习恒等(identity)函数.   
5. 迁移学习-transfer learning   
根据自己的样本数量的大小来确定冻结的层数,样本数量越大,你需要冻结的层数会越少.如果你有足够的样本量,你可以只使用下载好的权重作为初始化,而不使用随机初始化.  
6. 数据增广   
一般使用一个线程进行数据增广,将增广后得到的数据传递给其他的线程进行训练.数据增广的方式为:   
- mirror/crop/rotation/shearing/local warping   
- color shift   
7. 目标检测问题   
实质上是 classification + localization 问题.    
8. yolo 中将 ground truth box 分配到网格中的思路?   
计算 ground truth box 的中心,按照中心将物体投射到网格中.因此,不管在粗网格还是细网格中,即使横跨多个网格的一个目标也只会被投射到一个网格中.另外,多个物体被投射到一个网格中的概率也就很小了.    
9. IoU intersection of union   
交集/并集的比值.   
另外, IoU 还被用来表示两个 bounding box 之间的交集/并集的比值. 
10. NMS   
非极大值抑制之前会先将概率小于 0.6 的框移除. 然后再去除其他的重叠框.   
11. 什么是 SGD ?   
SGD 全称随机梯度下降法, 具体做法是每次从样本中随机抽取 1 个样本用来计算 Loss 函数, 将相应计算出的梯度作为当前这一步梯度下降的依据.   

## 学习中遇到的需要解释的问题   
1. 为什么不用一次函数,而用 ReLU 函数?   
如果用一次函数,那么神经网络不管有多少层,它都是一个线性变换矩阵.得到的结果是线性层面上的.   

4. 克服非凸优化问题的方法.    
加入冲量.   
SGD 方法.   
5. batch-size, iterations, epoches 之间的关系?   
full-batch: 所有的样本数量.    
batch-size: 每次计算梯度时选取的样本个数.   
iterations: 已经计算的梯度(梯度下降)次数.   
epoches: 遍历了所有训练样本数据集的次数.   
(batch-size * iterations) / full-batch = epoches.   
6. 什么样的数据是属于同一分布?   
图片质量, 图片场景, 目标拍摄角度等.
7. 数据均衡和类均衡采样   
数据均衡是不同类别的各自样本数量基本相等, 使用样本增广解决数据不均衡问题.    
类均衡采样是指每次随机选择 mini-batch 个数据中包含各个类别的数据数量是均匀的, 使用随机打乱 shuffle 来解决类均衡采样问题.   
8. 数据增广方式.  
直接在样本基础上做样本增广只是提高模型准确度的方法之一, 但是不是一个很好的方案, 因为这样增加的数据量有限, 并且还要占用额外的硬盘空间.    
最好的办法是训练的时候实时对数据进行增广, 这样就等效于无限多的随机增广.   
9. 分类和回归的区别是什么?   
回归是获取直线(轮廓), 分类是获取属于某一类别 label 的最大概率.  
分类用的是 SoftMaxWithLoss.   
回归用的是　EuclideanLoss, 计算真值和预测值之间的距离. 如果多个标签合在一起看成一个向量的话, 那么 Loss 函数就是这两个向量之间的欧式距离的平方(也可以看作是 L2 norm 范数).   
10. 有哪些目标检测算法?  
滑窗法: 穷举式搜索, 效率低并且需要考虑物体的长宽比.   
Selective Search(选择性搜索): 对图像中可能包含物体的区域进行搜索,进而提升物体检测的效率. 可能存在物体的区域都应该是有着某种相似性的或者连续的区域.  
不管是滑窗法还是 Selective Search , 这种找出可能包含物体的区域的方法, 统称为 Region Proposal.   
有了 Selective Search 高效率的寻找可能包含物体的方框, 那么接下来只要接个 CNN 提取特征, 然后做个分类不就相当于检测了.   
R-CNN: Region-based Convolutional Neural Network.   
RPN: 将 Selective Search 中传统图像算法用神经网络替代, 因为在 Fast R-CNN 中, Selective Search 成了限制计算效率提升的瓶颈.     
11. 有哪些物体检测的评价方法?  
IOU: 有时候未必合理, 视觉上重合度差不多的两个框, 实际应用中很可能因为分辨率不同得到差异很大的值.    


13. 为什么只正则化 w, 而没有加上参数 b ?   
因为 w 是一个高维参数向量, 特别是在 high variance 的情况下. 而 b 只是一个实数, 加上它对正则化也没有太多影响. 可以理解为只对仿射变化中的线性变换部分做了正则化处理.    
14. DropOut 中需要注意的地方.   
(1) DropOut 实现过程的最后需要对本层输出(下一层的输入)进行缩放(除以 keep_prop).   
(2) 在 test 阶段是不使用 DropOut 的.   
15. 为什么 Batch Norm 会起作用?   


## 工作中解决的重要问题
1. VOC 数据集对误检的影响   

2. 样本种类上去之后,如何单独处理 VOC 数据集和正常样本之间的test/train样本划分.  

3. 保证样本分布均匀的意义何在?   

4. 负样本比例较大会产生什么样的影响?    


## 深度学习岗位要求   

1. 负责AI平台图像算法包的设计与开发

任职要求：
1.熟悉深度学习的基本原理 DNN,CNN,RNN, 能熟练使用一种以上深度学习keras, tensorflow, theano, caffe， mxnet等
2.熟悉 Java 或者 Python 语言
3.有 Hadoop/Spark 相关项目开发经验者优先


## 解析卷积神经网络    
1. 为什么卷积在神经网络中如此受用?    
因为参数共享和稀疏链接, 卷积可使模型中参数数量大幅度减少.   
- 参数共享: 对图片一部分有作用的一个特征检测器可能在其他部分也有作用.    
- 稀疏链接: 卷积输出结果中的每一个值只依赖于输入图片中少量的像素值(比如3x3个角落值).  
另外, 卷积神经网络善于捕捉平移不变性.    
2. 小卷积核相比大卷积核的优点是什么?   
- 增强网络容量和模型复杂度.   
- 减少卷积参数的个数.   
3. 事实上, 卷积网络中的卷积核参数是通过网络训练学出来的. 一般选取卷积核大小 =3 和 pooling 核的大小 =2.   
4. pooling 层的作用    
- 特征不变性. 使模型更关注是否存在某些特征而不是特征具体的位置;   
- 特征降维. 下采样操作提取更广泛的特征;   
- 在一定程度上防止过拟合;    
选取 pooling 核的大小 =2 是为了不丢弃过多的输入响应而损失网络性能.    
5. 为什么 BN 可以缓解梯度"消失问题"?    
BN 可以固定每层输入信号的均值与方差. 这样一来, 即使网络模型较深层的响应或梯度很小,也可通过 BN 的规范化作用将其尺度变大, 以此便可解决深层网络训练很可能带来的"梯度消失"问题.  
6. 随机梯度下降法(SGD)和梯度下降法(full-batch)的区别 ?   
full-batch 是对所有样本计算一般之后求出梯度后下降一步, 而 SGD 只随机选取 1 个样本. SGD 的一个特点是计算速度快, 因此在计算效率上的差距是显而易见的.   
SGD 的另一个特点是随机性. 随机性像噪声一样带来不确定性, 但是在深度学习的应用中是一种优点, 因为深度学习中面临的基本都是非凸优化的问题. 在 SGD 中梯度有一定的概率是朝着"逃出"极小值的方向, 从而避免陷入极值. 同时梯度下降的大方向仍然保持正确.   
7. 数据不平衡问题的处理办法?   
数据层面    
- 数据重采样(上采样和下采样)    
下采样不是随机丢弃一部分图像, 因为那样会降低训练数据的多样性. 而是在批处理训练时随机抽取需要数量的图像. 仅使用数据上采样可能引起过拟合的问题, 因此有效的做法是将上采样和下采样结合使用进行数据重采样.   
- 类别平衡采样    
计算最大类别数 -> 对每一类生成随机数 -> 对该类类别数取余作为图像列表索引 -> 根据索引提取图像 -> 随机打乱(每类别的数目相同).   
算法层面    
- 代价敏感方法(根据样本比例和混淆矩阵指定代价敏感矩阵的权重值)    
9. mining-batch 梯度下降法和 SGD 梯度下降法.   
严格来说, mini-batch 和 SGD 不是一回事. 不过随着深度神经网络的房展, 现在一般提到 SGD, 其实多数是指 mini-batch.   
10. 网络每层的输入维度?   
一般在工程实践中, 由于采用了 mini-batch 训练策略, 网络第 L 层输入通常是一个四维张量, 即 `n*c*w*h` (n 为 mini-batch 每一批的样本数).   
11. 直接改变原始网络模型的输入图像分辨率可以吗?   
不可以, 会直接导致原始模型的卷积层的最终输出无法输入全连接, 此时需重新改变全连接层的输入滤波器的大小或重新指定其他相关参数.   
12. 为什么使用均值归一化?   
可以移除样本数据集中的共同部分, 凸显个体差异.    
13. 可以对参数进行全零初始化吗?   
不可以, 这样神经元将毫无能力对此做出改变, 从而无法进行模型训练.   
14. 过拟合是什么原因导致的? 解决过拟合的方法有哪些?  
权重太大导致网络有了记忆功能.  
- L1 , L2正则化.   
- DropOut 正则化.   
- 提前停止;  
- 数据扩充;   


4. 学习率使用轮数减缓, 指数减缓或分数减缓策略.   
6. BN 一般作用在非线性映射函数之前.    
激活函数的引入为的是增加整个网络的表达能力(非线性).   
参数初始化的时候需要特别注意, 避免初始化参数直接将输出值域带入导数为 0 的区域.    
目标函数的作用是用来衡量预测值与真实样本标记之间的误差; 交叉熵损失函数和 L2 损失函数分别是分类问题和回归问题中最为常见的目标函数.   
每个语义概念由许多分布在不同神经元中被激活的模式表示; 而每个神经元又可以参与到许多不同语义概念的表示中去.   
神经网络相应的区域多呈现"稀疏"特性, 即响应区域集中且占原图比例较小.   
深度特征具有层次性.   
数据随机 crop 以正方形抠取, 因为卷积神经网络模型的输入一般为正方形图像.   
均值归一化操作只针对训练集数据计算; 如果在训练时使用了均值归一化, 那么在验证和测试时也需要减去训练集上计算出的均值.  
在实际应用中, 服从高斯分布或均匀分布的随机参数都是较为有效的初始化方式.    
Xavier 参数初始化是考虑了网络输出数据分布的方差会随着输入神经元个数改变的影响, 在初始化的同时加上对方差大小的规范化.   
KL 散度用来衡量样本标记分布与真实标记分布之间的差异, 利用 KL 散度可以指导模型训练.    
L1 范数可以很好的区分恰好是 0 的元素和非零但是值很小的元素.   
L2 范数在原点附近增长的十分缓慢.   
dropout 正则化时, 在实际工程中, 会在训练阶段直接将随机失活后的网络响应乘以 1/(1-p)(p 为随机失活的概率) , 这样测试阶段便不需要做任何调整.   

AlexNet 特点?    
- 首次使用海量数据集;   
- 利用 GPU 实现网络训练;   
- 引入一些训练技巧, 如 ReLU 激活函数, 局部响应规范化(LRN, l2 规范化是其特例)操作, 数据增广和随机失活(dropout).  

VGG 特点?
- 普遍使用小卷积核;   
- 网络输出保持输入大小;   
- 卷积层的通道数逐渐增加(3 -> 64 -> 128 -> 256 -> 512);   

ResNet 特点?   
- 解决的问题: 随着网络深度的增加, 训练数据的训练误差没有降低反而升高;   
- highway network: 携带门和变换门均为恒等映射;   
- short cut 连接使梯度信息可以在多个神经网络之间有效传播;   
- 以 Global Average Polling 层替代 VGG 中的全连接层(一方面使得参数大大减少, 另一方面减少了过拟合风险);  